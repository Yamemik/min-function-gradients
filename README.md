# Минимизация функции

![Static Badge](https://img.shields.io/badge/Yamemik-gradients)
![GitHub top language](https://img.shields.io/github/languages/top/Yamemik/min-function-gradients)
![GitHub](https://img.shields.io/github/license/Yamemik/min-function-gradients)
![GitHub Repo stars](https://img.shields.io/github/stars/Yamemik/min-function-gradients)
![GitHub issues](https://img.shields.io/github/issues/Yamemik/min-function-gradients)


## Общее описание
_____
### Метод градиентного спуска с постоянным шагом
1. **Выбор начальной точки:** 
2. **Определение функции и градиента:** Определите функцию, которую нужно минимизировать, и её градиент, который будет вектором.
3. **Задание шага:** Установите значение шага (постоянный шаг).
4. **Обновление параметров:** На каждой итерации обновляйте значение вектора:
5. **Проверка сходимости:** Проверяйте условие сходимости. Обычно это делается по норме градиента или изменениям в функции:
6. **Повторение:** Повторяйте шаги 4 и 5 до достижения условия сходимости.

### Метод градиентного спуска с переменным шагом (Градиентный метод с дроблением параметра шага)

1. **Выбор начальной точки:** Выберите начальную точку.
2. **Определение функции и градиента:** Определите функцию, которую нужно минимизировать, и её градиент.
3. **Инициализация шага:** Установите начальное значение шага.
4. **Обновление параметров:** На каждой итерации обновляйте значение f(x)и шаг alpha:
   Обновление шага может быть выполнено с использованием различных стратегий:
   - **Дробление:** Уменьшение шага после каждой итерации, например alpha_{n+1} = alpha_n / 2.
   - **Адаптивные методы:** Использование методов, таких как Adam, Adagrad или RMSProp, которые автоматически настраивают шаг.
   - **Метод бэктрекинга:** Поиск подходящего шага, который уменьшает функцию, постепенно его уменьшая.
5. **Проверка сходимости:** Проверяйте условие сходимости.
6. **Повторение:** Повторяйте шаги 4 и 5, пока не достигнете условия сходимости.

### Метод наискорейшего спуска
1. **Определение точки:** 
2. **Определение функции и градиента:** Определите функцию $ f(x) и её градиент grad(fx).
3. **Направление спуска:** Находите направление спуска $ \mathbf{d} = -\nabla f(\mathbf{x}_0) $.

4. **Поиск шага с помощью метода золотого сечения:** С помощью этого метода находите оптимальное значение шага $ \alpha $.

5. **Обновление положения:** Обновляйте положение:
   \[
   \mathbf{x}_{n+1} = \mathbf{x}_n + \alpha \mathbf{d}
   \]

6. **Проверка сходимости:** Проверяйте условие сходимости.

7. **Повторение:** Повторяйте шаги 3–6, пока не будет достигнуто условие сходимости.

### Метод сопряжённых градиентов
1. **Квадратичная функция:** Метод сопряженных градиентов чаще всего используется для минимизации квадратичных функций вида:
   \[
   f(x) = \frac{1}{2} x^T A x - b^T x
   \]
   где $ A $ — симметричная положительно определённая матрица, $ b $ — вектор и $ x $ — искомый вектор.

2. **Идея метода:** Вместо того чтобы использовать градиенты по всем направлениям, метод использует так называемые "сопряженные направления" для быстрого сходимости к минимуму.

 Алгоритм метода сопряженных градиентов:

1. **Инициализация:**
   - Задайте начальное приближение $ x_0 $.
   - Вычислите начальный градиент: $ r_0 = b - Ax_0 $.
   - Установите начальное направление $ p_0 = r_0 $.

2. **Итерации:**
   Для $ k = 0, 1, 2, \ldots $:
   - Рассчитайте шаг:
     \[
     \alpha_k = \frac{r_k^T r_k}{p_k^T A p_k}
     \]
   - Обновите значение:
     \[
     x_{k+1} = x_k + \alpha_k p_k
     \]
   - Обновите остаток:
     \[
     r_{k+1} = r_k - \alpha_k A p_k
     \]
   - Проверьте условие сходимости (например, по норме остатка $ r_{k+1} $).
   - Рассчитайте новое направление Сопряжённого градиента:
     \[
     \beta_k = \frac{r_{k+1}^T r_{k+1}}{r_k^T r_k}
     \]
     \[
     p_{k+1} = r_{k+1} + \beta_k p_k
     \]

### Метод Ньютона
Метод Ньютона (или метод Ньютона-Рафсона) — это итеративный метод для нахождения корней функций, который также может быть применён для минимизации функций нескольких переменных. Этот метод использует информацию о градиенте (первых производных) и гессиане (вторых производных) для более точного нахождения точки минимума.

 Обновление точки выполняется по формуле:
   \[
   \mathbf{x}_{n+1} = \mathbf{x}_n - H(\mathbf{x}_n)^{-1} \nabla f(\mathbf{x}_n)
   \]