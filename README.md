# Минимизация функции

![Static Badge](https://img.shields.io/badge/Yamemik-gradients)
![GitHub top language](https://img.shields.io/github/languages/top/Yamemik/min-function-gradients)
![GitHub](https://img.shields.io/github/license/Yamemik/min-function-gradients)
![GitHub Repo stars](https://img.shields.io/github/stars/Yamemik/min-function-gradients)
![GitHub issues](https://img.shields.io/github/issues/Yamemik/min-function-gradients)


## Общее описание
_____
### Метод градиентного спуска с постоянным шагом
1. **Выбор начальной точки:** Выберите начальную точку $ \mathbf{x}_0 $ в пространстве, где вы хотите минимизировать функцию.

2. **Определение функции и градиента:** Определите функцию $ f(\mathbf{x}) $, которую нужно минимизировать, и её градиент $ \nabla f(\mathbf{x}) $, который будет вектором.

3. **Задание шага:** Установите значение шага $ \alpha $ (постоянный шаг).

4. **Обновление параметров:** На каждой итерации обновляйте значение вектора $ \mathbf{x} $:
   \[
   \mathbf{x}_{n+1} = \mathbf{x}_n - \alpha \nabla f(\mathbf{x}_n)
   \]

5. **Проверка сходимости:** Проверяйте условие сходимости. Обычно это делается по норме градиента или изменениям в функции:
   \[
   \|\nabla f(\mathbf{x}_n)\| < \epsilon
   \]
   или
   \[
   |f(\mathbf{x}_{n+1}) - f(\mathbf{x}_n)| < \epsilon
   \]

6. **Повторение:** Повторяйте шаги 4 и 5 до достижения условия сходимости.

### Метод градиентного спуска с переменным шагом (Градиентный метод с дроблением параметра шага)

1. **Выбор начальной точки:** Выберите начальную точку $ \mathbf{x}_0 $.

2. **Определение функции и градиента:** Определите функцию $ f(\mathbf{x}) $, которую нужно минимизировать, и её градиент $ \nabla f(\mathbf{x}) $.

3. **Инициализация шага:** Установите начальное значение шага $ \alpha_0 $.

4. **Обновление параметров:** На каждой итерации обновляйте значение $ \mathbf{x} $ и шаг $ \alpha $:
   \[
   \mathbf{x}_{n+1} = \mathbf{x}_n - \alpha_n \nabla f(\mathbf{x}_n)
   \]
   Обновление шага может быть выполнено с использованием различных стратегий:

   - **Дробление:** Уменьшение шага после каждой итерации, например $ \alpha_{n+1} = \alpha_n / 2 $.
   - **Адаптивные методы:** Использование методов, таких как Adam, Adagrad или RMSProp, которые автоматически настраивают шаг.
   - **Метод бэктрекинга:** Поиск подходящего шага, который уменьшает функцию, постепенно его уменьшая.

5. **Проверка сходимости:** Проверяйте условие сходимости.

6. **Повторение:** Повторяйте шаги 4 и 5, пока не достигнете условия сходимости.

### Метод наискорейшего спуска
1. **Определение точки:** Начальная точка $ \mathbf{x}_0 $.

2. **Определение функции и градиента:** Определите функцию $ f(\mathbf{x}) $ и её градиент $ \nabla f(\mathbf{x}) $.

3. **Направление спуска:** Находите направление спуска $ \mathbf{d} = -\nabla f(\mathbf{x}_0) $.

4. **Поиск шага с помощью метода золотого сечения:** С помощью этого метода находите оптимальное значение шага $ \alpha $.

5. **Обновление положения:** Обновляйте положение:
   \[
   \mathbf{x}_{n+1} = \mathbf{x}_n + \alpha \mathbf{d}
   \]

6. **Проверка сходимости:** Проверяйте условие сходимости.

7. **Повторение:** Повторяйте шаги 3–6, пока не будет достигнуто условие сходимости.

### Метод сопряжённых градиентов
### Метод Ньютона
